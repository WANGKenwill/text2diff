from typing import List, Tuple
import ell
from src.core import MODEL_NAME

import re
import json

def split_into_sentences(paragraph):
    # Regular expression pattern
    sentence_endings = r'(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.|\?|\!)\s'
    sentences = re.split(sentence_endings, paragraph)    
    return sentences

def split_text_by_sentences3(text: str) -> List[Tuple[int, str]]:
    """
    æŒ‰å¥å­æˆ–åŒæ¢è¡Œç¬¦åˆ†å‰²æ–‡æœ¬ï¼Œå¹¶è¿”å›æ¯å—çš„æ–‡æœ¬åŠå…¶åœ¨åŸå§‹å­—ç¬¦ä¸²ä¸­çš„èµ·å§‹ä½ç½®ã€‚
    åˆ†éš”ç¬¦ï¼ˆå¦‚å¥å·ã€æ„Ÿå¹å·ã€é—®å·ï¼‰ä¼šä¿ç•™åœ¨æ¯å¥çš„æœ«å°¾ã€‚
    
    Args:
        text (str): åŸå§‹æ–‡æœ¬
    
    Returns:
        List[Tuple[int, str]]: æ¯å—çš„èµ·å§‹ä½ç½®å’Œæ–‡æœ¬
    """
    # ä¼˜åŒ–åçš„æ­£åˆ™è¡¨è¾¾å¼
    separator_pattern = r'(?:[^ã€‚ï¼ï¼Ÿ]*[ã€‚ï¼ï¼Ÿ]|[^.!?]*[.!?](?=\s|$))(?:\s*["\']?)?|\n\s*\n'
    matches = re.finditer(separator_pattern, text)
    result = []
    
    for match in matches:
        start_pos = match.start()
        chunk = match.group().strip()
        if chunk:  # ç¡®ä¿ä¸æ·»åŠ ç©ºå­—ç¬¦ä¸²
            result.append((start_pos, chunk))
    
    return result

def split_text_by_sentences2(text: str) -> List[Tuple[int, str]]:
    """
    æŒ‰å¥å­æˆ–åŒæ¢è¡Œç¬¦åˆ†å‰²æ–‡æœ¬ï¼Œå¹¶è¿”å›æ¯å—çš„æ–‡æœ¬åŠå…¶åœ¨åŸå§‹å­—ç¬¦ä¸²ä¸­çš„èµ·å§‹ä½ç½®ã€‚
    åˆ†éš”ç¬¦ï¼ˆå¦‚å¥å·ï¼‰ä¼šä¿ç•™åœ¨æ¯å¥çš„æœ«å°¾ã€‚

    Args:
        text (str): åŸå§‹æ–‡æœ¬

    Returns:
        List[Tuple[int, str]]: æ¯å—çš„èµ·å§‹ä½ç½®å’Œæ–‡æœ¬
    """
    # æ­£åˆ™è¡¨è¾¾å¼ï¼šåŒ¹é…å¥å­ç»“æŸç¬¦å·æˆ–åŒæ¢è¡Œç¬¦ï¼Œä¿ç•™åˆ†éš”ç¬¦
    separator_pattern = r"(.+?[.!?]|\n\n)(?:\s+|$)"
    matches = re.findall(separator_pattern, text, flags=re.DOTALL)
    
    # è®¡ç®—æ¯å—çš„èµ·å§‹ä½ç½®
    result = []
    current_pos = 0
    for chunk in matches:
        result.append((current_pos, chunk))
        current_pos += len(chunk)
    
    return result


def rebuild_text_with_positions(chunks_with_starts: List[Tuple[str, int]]) -> str:
    """
    å°†åˆ†å‰²åçš„æ–‡æœ¬å—é‡æ–°æ‹¼æ¥ï¼Œå¹¶åœ¨æ¯å—å‰æ’å…¥èµ·å§‹ä½ç½®æ ‡è®°ã€‚

    Args:
        chunks_with_starts (List[Tuple[str, int]]): æ¯å—çš„æ–‡æœ¬åŠå…¶èµ·å§‹ä½ç½®

    Returns:
        str: é‡æ–°æ‹¼æ¥åçš„æ–‡æœ¬ï¼ŒåŒ…å«èµ·å§‹ä½ç½®æ ‡è®°
    """
    result = []
    for start, chunk in chunks_with_starts:
        # æ’å…¥èµ·å§‹ä½ç½®æ ‡è®°
        result.append(f"â€–start:{start}â€–{chunk}")
    return "".join(result)






@ell.simple(model=MODEL_NAME)
def text2diff_llm(text_with_pos:str, revision_text:str):
    """
ä½ çš„ä»»åŠ¡æ˜¯åŸºäºåŸæ–‡å’Œä¿®æ”¹å»ºè®®ç”Ÿæˆä¿®æ”¹æ•°æ®ç»“æ„ã€‚

å…³äºä½ç½®æ ‡è®°ï¼š
- åŸæ–‡ä¸­çš„â€–start:æ•°å­—â€–æ˜¯ä½ç½®æ ‡è®°ç¬¦ï¼Œæ ‡è®°äº†åç»­æ–‡æœ¬çš„èµ·å§‹ä½ç½®
- åœ¨å®šä½originalå†…å®¹æ—¶ï¼Œä½¿ç”¨è¯¥å†…å®¹å¾€å‰è¿½æº¯çš„æœ€è¿‘ä¸€ä¸ªâ€–start:æ•°å­—â€–ä¸­çš„æ•°å­—ä½œä¸ºstartå€¼
- ä¸¥æ ¼ä½¿ç”¨æ ‡è®°ç¬¦ä¸­çš„æ•°å­—ï¼Œä¸è¦è‡ªè¡Œè®¡ç®—æˆ–ä¼°ç®—ä½ç½®

è¾“å‡ºæ ¼å¼ï¼š
[
    {

      "sentence_start": <ä½¿ç”¨originalå†…å®¹å¾€å‰è¿½æº¯çš„æœ€è¿‘ä¸€ä¸ªä½ç½®æ ‡è®°ç¬¦é‡Œçš„æ•°å­—>,
      "original": <éœ€è¦ä¿®æ”¹çš„åŸæ–‡>,
      "content": <ä¿®æ”¹åçš„å†…å®¹>


    }
]


ç¤ºä¾‹åŸæ–‡ï¼š
â€–start:0â€–è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å¥å­â€–start:6â€–è¿™æ˜¯ç¬¬äºŒä¸ªæµ‹è¯•å¥å­ã€‚

ç¤ºä¾‹ä¿®æ”¹å»ºè®®ï¼š
æŠŠ"ç¬¬äºŒä¸ª"æ”¹ä¸º"ç¬¬ä¸‰ä¸ª"

ç¤ºä¾‹è¾“å‡ºï¼š
[
    {
        "sentence_start": 6,
        "original": "ç¬¬äºŒä¸ª",
        "content": "ç¬¬ä¸‰ä¸ª"
    }
]

æ³¨æ„ï¼š
1. ä¸¥æ ¼ä½¿ç”¨æ ‡è®°ç¬¦ä¸­çš„æ•°å­—ä½œä¸ºstartå€¼ï¼Œä¸è¦è®¡ç®—
2. ä½ç½®æ ‡è®°ç¬¦ä¸æ˜¯æ–‡æœ¬å†…å®¹çš„ä¸€éƒ¨åˆ†ï¼Œä¸è¦å°†å…¶åŒ…å«åœ¨originalä¸­
"""
    return f"ç°åœ¨è¯·å¤„ç†ä»¥ä¸‹å†…å®¹ï¼š\nåŸæ–‡ï¼š\n{text_with_pos}\nä¿®æ”¹å»ºè®®ï¼š\n{revision_text}"

def apply_diff(origin_text: str, replacements: list) -> str:
    """
    åº”ç”¨ä¿®æ”¹åˆ°åŸå§‹æ–‡æœ¬ã€‚
    """

    # 1. æŒ‰ç…§ start ä½ç½®é™åºæ’åºï¼Œä»åå¾€å‰æ›¿æ¢
    sorted_replacements = sorted(replacements, key=lambda x: x[0], reverse=True)
    
    # 2. ä¾æ¬¡åº”ç”¨æ¯ä¸ªä¿®æ”¹
    result = origin_text
    for start, end, content in sorted_replacements:
        result = result[:start] + content + result[end:]
    
    return result


def diff2md(origin_text: str, replacements: List[Tuple[int, int, str]]) -> str:
    """
    å°†ä¿®æ”¹ä¿¡æ¯æ¸²æŸ“ä¸ºMarkdownæ ¼å¼
    
    Args:
        origin_text (str): åŸå§‹æ–‡æœ¬
        revisions (List[Tuple[int, int, str]]): ä¿®æ”¹åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ (start, end, content)
        
    Returns:
        str: Markdownæ ¼å¼çš„ä¿®æ”¹ä¿¡æ¯
    """
    # 1. æŒ‰ç…§startä½ç½®å‡åºæ’åº
    sorted_replacements = sorted(replacements, key=lambda x: x[0])
    
    # 2. æ„å»ºMarkdownå†…å®¹
    md_lines = []
    current_pos = 0
    
    for start, end, content in sorted_replacements:
        # æ·»åŠ æœªä¿®æ”¹éƒ¨åˆ†
        if current_pos < start:
            md_lines.append(origin_text[current_pos:start])
        
        # æ·»åŠ ä¿®æ”¹éƒ¨åˆ†
        original = origin_text[start:end]
        md_lines.append(f"~~{original}~~ â†’ **{content}**")
        
        current_pos = end
    
    # æ·»åŠ æœ€åæœªä¿®æ”¹éƒ¨åˆ†
    if current_pos < len(origin_text):
        md_lines.append(origin_text[current_pos:])
    
    return "".join(md_lines)

def diff2html(origin_text: str, replacements: List[Tuple[int, int, str]]) -> str:
    """
    å°†ä¿®æ”¹ä¿¡æ¯æ¸²æŸ“ä¸ºHTMLæ ¼å¼
    
    Args:
        origin_text (str): åŸå§‹æ–‡æœ¬
        replacements (List[Tuple[int, int, str]]): ä¿®æ”¹åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ (start, end, content)
        
    Returns:
        str: HTMLæ ¼å¼çš„ä¿®æ”¹ä¿¡æ¯
    """
    # 1. æŒ‰ç…§startä½ç½®å‡åºæ’åº
    sorted_replacements = sorted(replacements, key=lambda x: x[0])
    
    # 2. æ„å»ºHTMLå†…å®¹
    html_lines = []
    current_pos = 0
    
    for start, end, content in sorted_replacements:
        # æ·»åŠ æœªä¿®æ”¹éƒ¨åˆ†
        if current_pos < start:
            html_lines.append(origin_text[current_pos:start])
        
        # æ·»åŠ ä¿®æ”¹éƒ¨åˆ†
        original = origin_text[start:end]
        html_lines.append(
            f"<span style='color:red;text-decoration:line-through'>{original}</span>"
            f" â†’ "
            f"<span style='color:green;font-weight:bold'>{content}</span>"
        )
        
        current_pos = end
    
    # æ·»åŠ æœ€åæœªä¿®æ”¹éƒ¨åˆ†
    if current_pos < len(origin_text):
        html_lines.append(origin_text[current_pos:])
    
    return "".join(html_lines)

def exact_revision(origin_text: str, revisions: dict) -> List[Tuple[int, int, str]]:
    """
    è·å–ç²¾ç¡®çš„æ›¿æ¢ä½ç½®ï¼Œè¿”å›ä¸€ä¸ªåˆ—è¡¨ï¼Œå…ƒç´ æ˜¯ (start, end, content) ç»„æˆçš„å…ƒç»„ã€‚

    Args:
        origin_text (str): åŸå§‹æ–‡æœ¬
        revisions (dict): ä¿®æ”¹å†…å®¹ï¼ŒåŒ…å« "revisions" å­—æ®µ

    Returns:
        List[Tuple[int, int, str]]: æ›¿æ¢ä½ç½®åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ (start, end, content)
    """
    result = []
    for rev in revisions:
        start = rev["sentence_start"]
        original = rev["original"]
        content = rev["content"]
        
        # ä» start ä½ç½®å¼€å§‹æœç´¢ original
        match_pos = origin_text.find(original, start)
        if match_pos != -1:
            # å¦‚æœæ‰¾åˆ°åŒ¹é…çš„ originalï¼Œè®°å½•æ›¿æ¢ä½ç½®
            end = match_pos + len(original)
            result.append((match_pos, end, content))
    
    return result



def build_text_with_pos(text):
    result = split_text_by_sentences(text)
    return rebuild_text_with_positions(result)

def text2diff(origin_text:str, revision_text:str):
    result = build_text_with_pos(origin_text)
    resp = text2diff_llm(result, revision_text)
    return exact_revision(origin_text, json.loads(resp))


def split_text_by_sentences(text: str) -> List[Tuple[int, str]]:
    """
    æŒ‰å¥å­åˆ†å‰²æ–‡æœ¬ï¼Œè¿”å›æ¯ä¸ªå¥å­åŠå…¶èµ·å§‹ä½ç½®ã€‚
    ä¿ç•™å¥æœ«æ ‡ç‚¹ï¼Œç‰¹æ®Šå¤„ç†é‚®ç®±å’Œç½‘å€ä¸­çš„ç‚¹å·ã€‚
    
    Args:
        text (str): åŸå§‹æ–‡æœ¬
    Returns:
        List[Tuple[int, str]]: æ¯å¥çš„èµ·å§‹ä½ç½®å’Œå†…å®¹
    """
    # æ­£åˆ™è¡¨è¾¾å¼ï¼šåŒ¹é…å®Œæ•´å¥å­ï¼Œç‚¹å·åå¿…é¡»æ˜¯ç©ºç™½æˆ–ç»“æŸ
    # separator_pattern = r'[^ã€‚ï¼ï¼Ÿ!?]+?(?:\.(?=\s|$)|[ã€‚ï¼ï¼Ÿ!?])[\s]*'
    # æš‚æ—¶ç”¨åˆ°è¿™é‡Œã€‚ï¼Œç°åœ¨å°±ç”¨ä¾‹5æ²¡è¿‡ï¼Œ.å’Œ"åŒæ—¶åœ¨çš„æ—¶å€™ä¼šæœ‰é—®é¢˜
    # separator_pattern = r'[^ã€‚ï¼ï¼Ÿ!?]+?(?:\.(?=\s|$|[\'"]\s)|[ã€‚ï¼ï¼Ÿ!?])[\s]*'
    separator_pattern = r'[^ã€‚ï¼ï¼Ÿ!?]+?(?:\.(?=\s|$|[\'"]\s*)|[ã€‚ï¼ï¼Ÿ!?])[\s]*'

    matches = re.finditer(separator_pattern, text)
    result = []
    
    for match in matches:
        chunk = match.group()
        if chunk:  # ç¡®ä¿ä¸æ·»åŠ ç©ºå­—ç¬¦ä¸²
            result.append((match.start(), chunk))
    
    return result

# æµ‹è¯•ç”¨ä¾‹
def test_sentence_splitter():
    test_cases = [
        # 1. åŸºæœ¬è‹±æ–‡å¥å­
        (
            "Hello world. This is a test! How are you?",
            [(0, "Hello world. "), (13, "This is a test! "), (29, "How are you?")]
        ),
        
        # 2. åŒ…å«é‚®ç®±
        (
            "My email is test.name@example.com. Next sentence!",
            [(0, "My email is test.name@example.com. "), (35, "Next sentence!")]
        ),
        
        # 3. åŒ…å«ç½‘å€
        (
            "Visit www.example.com. Then contact us!",
            [(0, "Visit www.example.com. "), (23, "Then contact us!")]
        ),
        
        # 4. åŒ…å«çœç•¥å·
        (
            "Think about it... Then decide!",
            [(0, "Think about it... "), (18, "Then decide!")]
        ),
        
        # 5. å¸¦å¼•å·çš„å¯¹è¯
        (
            'He said "Hello." She replied "Hi!"',
            [(0, 'He said "Hello." '), (16, 'She replied "Hi!"')]
        ),
        
        # 6. ç‰ˆæœ¬å·
        (
            "Using version 2.0.1. Next line.",
            [(0, "Using version 2.0.1. "), (21, "Next line.")]
        ),
        
        # 7. æ··åˆä¸­è‹±æ–‡
        (
            "Helloä¸–ç•Œã€‚Thisæ˜¯test.comç½‘ç«™ã€‚Good!",
            [(0, "Helloä¸–ç•Œã€‚"), (8, "Thisæ˜¯test.comç½‘ç«™ã€‚"), (24, "Good!")]
        )
    ]

    passed_count = 0
    failed_count = 0

    print("\nå¼€å§‹æ‰§è¡Œæµ‹è¯•ç”¨ä¾‹...\n")
    for i, (input_text, expected) in enumerate(test_cases, 1):
        try:
            result = split_text_by_sentences(input_text)
            
            # æ–°å¢ï¼šéªŒè¯é‡æ–°æ‹¼æ¥æ˜¯å¦ç­‰äºåŸæ–‡
            reconstructed_text = ''.join([chunk for _, chunk in result])
            reconstruction_match = reconstructed_text == input_text
            
            if result == expected and reconstruction_match:
                print(f"âœ… æµ‹è¯•ç”¨ä¾‹ {i} é€šè¿‡")
                passed_count += 1
            else:
                print(f"âŒ æµ‹è¯•ç”¨ä¾‹ {i} å¤±è´¥")
                print(f"   è¾“å…¥: {input_text}")
                print(f"   é¢„æœŸ: {expected}")
                print(f"   å®é™…: {result}")
                
                if not reconstruction_match:
                    print(f"   é‡æ–°æ‹¼æ¥ä¸åŒ¹é…ï¼")
                    print(f"   åŸæ–‡é•¿åº¦: {len(input_text)}")
                    print(f"   æ‹¼æ¥é•¿åº¦: {len(reconstructed_text)}")
                    print(f"   åŸæ–‡: {input_text}")
                    print(f"   æ‹¼æ¥: {reconstructed_text}")
                print()
                
                failed_count += 1
        except Exception as e:
            print(f"âš ï¸ æµ‹è¯•ç”¨ä¾‹ {i} å‡ºç°å¼‚å¸¸")
            print(f"   é”™è¯¯ä¿¡æ¯: {str(e)}")
            failed_count += 1
    
    print(f"\næµ‹è¯•ç»“æœï¼šé€šè¿‡ {passed_count} ä¸ªï¼Œå¤±è´¥ {failed_count} ä¸ª")
    if failed_count == 0:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹é€šè¿‡ï¼")
    else:
        print("ğŸ’” å­˜åœ¨å¤±è´¥çš„æµ‹è¯•ç”¨ä¾‹")


if __name__ == "__main__":

    # åŸæ–‡ç¤ºä¾‹
    text = """AIæ­£åœ¨æ·±åˆ»æ”¹å˜æˆ‘ä»¬çš„ä¸–ç•Œã€‚ä»åŒ»ç–—è¯Šæ–­åˆ°è‡ªåŠ¨é©¾é©¶ï¼ŒAIæŠ€æœ¯æ­£åœ¨å„ä¸ªé¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ã€‚
åœ¨åŒ»ç–—é¢†åŸŸï¼ŒAIå¯ä»¥å¸®åŠ©åŒ»ç”Ÿæ›´å‡†ç¡®åœ°è¯Šæ–­ç–¾ç—…ã€‚åœ¨äº¤é€šé¢†åŸŸï¼Œè‡ªåŠ¨é©¾é©¶æŠ€æœ¯æœ‰æœ›å‡å°‘äº¤é€šäº‹æ•…ã€‚
ç„¶è€Œï¼ŒAIçš„å‘å±•ä¹Ÿå¸¦æ¥äº†ä¸€äº›æŒ‘æˆ˜ã€‚æˆ‘ä»¬éœ€è¦ç¡®ä¿AIçš„ä½¿ç”¨æ˜¯è´Ÿè´£ä»»çš„ï¼Œç¬¦åˆä¼¦ç†è§„èŒƒã€‚
ä¸“å®¶ä»¬è®¤ä¸ºï¼ŒAIåº”è¯¥æœåŠ¡äºäººç±»ï¼Œè€Œä¸æ˜¯å–ä»£äººç±»ã€‚æœªæ¥ï¼ŒAIå°†ä¸äººç±»ååŒå·¥ä½œï¼Œåˆ›é€ æ›´ç¾å¥½çš„ä¸–ç•Œã€‚"""


    replacements = text2diff(text, """
    1. æŠŠ"æˆ‘ä»¬çš„"æ”¹æˆ"å¤§å®¶çš„"
    2. åœ¨"åŒ»ç–—è¯Šæ–­"å‰æ·»åŠ "å…ˆè¿›çš„"
    3. åœ¨"äº¤é€šäº‹æ•…"åæ·»åŠ "ï¼Œæé«˜é“è·¯å®‰å…¨"
    4. åˆ é™¤"ä¸€äº›"
    """)

    print(f"\n{replacements}")

    revision = apply_diff(text, replacements)
    print(f"\n{revision}")

    # test_sentence_splitter()
